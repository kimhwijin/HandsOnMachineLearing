{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/HandsOnMachineLearing/blob/main/NLP_RNN_and_Attention_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tinRW98JrHMf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "m48vYq7Br8rA"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare\", dataset_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjs7iwDjsZ0h",
        "outputId": "1f209029-1950-4806-a73a-024ec9f041df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(shakespeare_text[:148])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZUC09-L5sqa6",
        "outputId": "a7351f31-8139-43e0-9f60-c383d84b2ffb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "\"\".join(sorted((set(shakespeare_text.lower()))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEnEbJTVe2G2"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "X3nTL63yuwIz"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ukl_rAiu2vh",
        "outputId": "200416f2-a424-4ac2-db97-bb69999dc9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'i': 6, 'h': 7, 's': 8, 'r': 9, 'n': 10, '\\n': 11, 'l': 12, 'd': 13, 'u': 14, 'm': 15, 'y': 16, 'w': 17, ',': 18, 'c': 19, 'f': 20, 'g': 21, 'b': 22, 'p': 23, ':': 24, 'k': 25, 'v': 26, '.': 27, \"'\": 28, ';': 29, '?': 30, '!': 31, '-': 32, 'j': 33, 'q': 34, 'x': 35, 'z': 36, '3': 37, '&': 38, '$': 39}\n",
            "[[8, 7, 5, 25, 2, 8, 23, 2, 5, 9, 2, 2]]\n",
            "['s h a k e s p e a r e']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.word_index)\n",
        "print(tokenizer.texts_to_sequences([\"Shakespeare@e\"]))\n",
        "print(tokenizer.sequences_to_texts([[8, 7, 5, 25, 2, 8, 23, 2, 5, 9, 2]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo1IwAQwwpI7",
        "outputId": "ca73998e-a777-440f-a4e3-2447729d6e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "1115394\n",
            "1115394\n"
          ]
        }
      ],
      "source": [
        "max_id = len(tokenizer.word_index)\n",
        "print(max_id)\n",
        "dataset_size = tokenizer.document_count\n",
        "print(dataset_size)\n",
        "print(sum(tokenizer.word_counts.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJn694OsexA7"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "L6Gc4yqkxkXl"
      },
      "outputs": [],
      "source": [
        "#index가 1 부터시작해서 -1 을 해줘서 0 부터로 조정함\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "buNGc3WYyCsz"
      },
      "outputs": [],
      "source": [
        "#수백만개의 1D 차원 시퀀스 데이터를 window() 메서드로 작은 많은 텍스트로 변환한다.\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = 다음 1 글자 input\n",
        "#shift = 1 이면, 데이터셋을 꽉차게 사용한다. 0~100 , 1~101 , ...\n",
        "#window는 데이터셋을 만들어서, 리스트의 리스트 같이, 중첩 데이터셋을 만듬\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4LtD8Q-a8al",
        "outputId": "b101dba7-ad57-42e8-ef52-a1bf0f6ffd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 <_VariantDataset shapes: (), types: tf.int64>\n"
          ]
        }
      ],
      "source": [
        "for a in dataset.take(1):\n",
        "    #데이터셋\n",
        "    print(len(a), a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OscR6hSSboGl",
        "outputId": "3336350e-b7d4-4da9-bbaf-5aff9c196bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 tf.Tensor(\n",
            "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
            "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
            "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
            " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
            " 10 15  3 13  0], shape=(101,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "#중첩 데이터셋을 덴서를 포함한 데이터셋으로 변경함.\n",
        "dataset = dataset.flat_map(lambda window : window.batch(window_length))\n",
        "for a in dataset.take(1):\n",
        "    print(len(a), a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "nlyowzQldW0B"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "#이전 100개의 글자와 타깃값 1글자를 분리함\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DNPjtn9nd8yU"
      },
      "outputs": [],
      "source": [
        "#각 글자를 one hot 벡터로 치환한다.\n",
        "#원래는 글자수가 많으면 임베딩을 사용함\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTnhhvmzf7Mk",
        "outputId": "9696160e-bed4-4b39-83c5-735265b7ad19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(100, 39), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 5  7  0  7  4 15  7  0  2  6  1  0 21  1 11 11 15 17  0 14  4  8 24  0\n",
            " 14  1 17 31 31 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10  4 15\n",
            " 17  0  7  5  8 28  0 16  1 11 11 17  0 16  1 11 11 26 10 10 14  1  9  1\n",
            "  9  5 13  7 23 10 27  2  6  3 13 20  6  0  4 11 11  0  4  2  0  3  9 18\n",
            "  1  0 18  4], shape=(100,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for x, y in dataset.take(1):\n",
        "    print(x[0])\n",
        "    print(y[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU1E5liXe5uM"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pDvS51Gaeudb"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                     dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_path = '/content/drive/MyDrive/Model/shakespeare/'\n",
        "model_name = 'shakespeare'\n",
        "model_path = model_path + model_name\n",
        "\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    history = model.fit(dataset, epochs=10)\n",
        "    model.save(model_path)\n",
        "    print('-'*20)\n",
        "else:\n",
        "    model = tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "Gph1a03tA47U"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TextGenerate"
      ],
      "metadata": {
        "id": "QfasCIMp31xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)\n",
        "\n",
        "X_new = preprocess([\"who are yo\"])\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "print(\"who are yo\" + tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]) # 1st sentence, last char\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJRFwUJZ25CD",
        "outputId": "c22cdc7a-b995-4070-b4bd-69d24ba4a353"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "who are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "    #to one_hot arrays\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "SwwkH3l6_I1A"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"i\", n_chars=100, temperature=0.2))\n",
        "print('---')\n",
        "print(complete_text(\"i\", n_chars=100, temperature=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyuzjnSaAkMx",
        "outputId": "342caca9-7d1e-43ca-96cc-97a228833935"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in the rest was so beheld and hearting and my father raiment for the rest words signior gremio.\n",
            "\n",
            "grum\n",
            "---\n",
            "ice?\n",
            "\n",
            "petruchio:\n",
            "bepake her burst in all in happy eneep,\n",
            "that s sasder it, sir, yet myself as's solen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StatefulRNN"
      ],
      "metadata": {
        "id": "EuKFVicR5KTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.batch(1)\n",
        "dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "nRTmhsNpxtHe"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StatefullRNN_Model"
      ],
      "metadata": {
        "id": "Tq4mykHI7NIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, batch_input_shape=[batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])"
      ],
      "metadata": {
        "id": "dxPzEpiq6FO7"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CallBack"
      ],
      "metadata": {
        "id": "sP5fhTSk70ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ],
      "metadata": {
        "id": "4YKw-wIo74hz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StatefullTrain"
      ],
      "metadata": {
        "id": "rr4jv5tW782E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "43y27esG78O_"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_path = '/content/drive/MyDrive/Model/shakespeare/'\n",
        "model_name = 'shakespeare_stateful'\n",
        "model_path = model_path + model_name\n",
        "\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path)\n",
        "    history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])\n",
        "    model.save(model_path)\n",
        "    print('-'*20)\n",
        "else:\n",
        "    model = tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "Qp2LUqO68VUn"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment_Analysis"
      ],
      "metadata": {
        "id": "K-mu_cG7lald"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
      ],
      "metadata": {
        "id": "zcCLL12LlIqU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0][:10])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_dKmGi5l3iS",
        "outputId": "fd333d5f-ebfa-498f-ff7d-8506752cf74f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "#0,1,2 토큰은 패딩토큰, SOS start_or_sequence 토큰, 알수없는 단어를 의미하는 토큰 이다.\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in X_train[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "lRFzAibkmBhI",
        "outputId": "ec799288-09c1-4909-951c-122968f8e011"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<sos> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#전처리되지 않은 원본 리뷰데이터\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
      ],
      "metadata": {
        "id": "QsRNpmWNm8c_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = info.splits['train'].num_examples\n",
        "test_size = info.splits['test'].num_examples\n",
        "print(*datasets.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHb69LL5pSJ3",
        "outputId": "39c87d02-532a-48b9-bbd2-5b7d9ef6994e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test train unsupervised\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in datasets['train'].take(2):\n",
        "    review = X.numpy()\n",
        "    label = y.numpy()\n",
        "    print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "    print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjzz9DkxpqbC",
        "outputId": "65c821dd-7379-455d-97ec-7a237ed561e3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "vy8fivFLUtbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    #300 글자 제한\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    #<br /> 태그를 공백으로\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
        "    #a~Z 가 아니면 공백으로\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    #공백 기준으로 분리\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
        "\n",
        "for X_batch, y_batch in datasets['train'].batch(3).take(1):\n",
        "    print(preprocess(X_batch, y_batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIf_R6vIpFhz",
        "outputId": "046912be-59c7-4b01-ad05-90e8045c32c5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(3, 53), dtype=string, numpy=\n",
            "array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
            "        b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
            "        b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
            "        b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
            "        b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
            "        b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
            "        b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
            "        b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
            "        b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
            "       [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
            "        b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
            "        b'to', b'a', b'combination', b'of', b'things', b'including',\n",
            "        b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
            "        b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
            "        b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
            "        b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
            "        b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
            "        b'Cons'],\n",
            "       [b'Mann', b'photographs', b'the', b'Alberta', b'Rocky',\n",
            "        b'Mountains', b'in', b'a', b'superb', b'fashion', b'and',\n",
            "        b'Jimmy', b'Stewart', b'and', b'Walter', b'Brennan', b'give',\n",
            "        b'enjoyable', b'performances', b'as', b'they', b'always',\n",
            "        b'seem', b'to', b'do', b'But', b'come', b'on', b'Hollywood',\n",
            "        b'a', b'Mountie', b'telling', b'the', b'people', b'of',\n",
            "        b'Dawson', b'City', b'Yukon', b'to', b'elect', b'themselves',\n",
            "        b'a', b'marshal', b'yes', b'a', b'marshal', b'and', b'to', b'e',\n",
            "        b'<pad>', b'<pad>', b'<pad>', b'<pad>']], dtype=object)>, <tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 0, 0])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))\n",
        "\n",
        "print(vocabulary.most_common()[:10])\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em1WbOIXrvjG",
        "outputId": "eeb1b04b-93cd-47d1-d5bb-41ae6c5da3a5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564), (b'of', 33983), (b'and', 33431), (b'to', 27707), (b'I', 27019), (b'is', 25719), (b'in', 18966), (b'this', 18490)]\n",
            "53893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#가장 많이 등장하는 단어 개수\n",
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
        "]\n",
        "word_to_id = {word : index for index, word in enumerate(truncated_vocabulary)}"
      ],
      "metadata": {
        "id": "U0tHs89zsjR-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets=num_oov_buckets)"
      ],
      "metadata": {
        "id": "Y3O3s87HtheD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table.lookup(tf.constant([b\"this is so crazzzzy\".split()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWGAZ1YtuI0-",
        "outputId": "1eb695a7-5fd4-410d-cc87-084e801151df"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[    9,     7,    34, 10991]])>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#table\n",
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch"
      ],
      "metadata": {
        "id": "GvoLMxUOuinl"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets['train'].batch(32).map(preprocess).map(encode_words).prefetch(1)"
      ],
      "metadata": {
        "id": "sS4yP_Uhuttp"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nwKEL6Wu_ne",
        "outputId": "dbfd3f5c-8c76-4593-cde1-273e448ae55c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 12s 12ms/step - loss: 0.6263 - accuracy: 0.6124\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.3850 - accuracy: 0.8296\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2194 - accuracy: 0.9166\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.1406 - accuracy: 0.9513\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.1047 - accuracy: 0.9636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#API_Masking"
      ],
      "metadata": {
        "id": "81VMnTjVhHAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#<pad> 토큰을 제외하고 학습\n",
        "K = keras.backend\n",
        "embed_size = 128\n",
        "inputs = keras.layers.Input(shape=[None])\n",
        "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
        "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
        "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
        "z = keras.layers.GRU(128)(z)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "id": "i72KwGebxMp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997eef6c-a921-470d-e11e-8b664f6a5b02"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 20s 17ms/step - loss: 0.5361 - accuracy: 0.7254\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.3509 - accuracy: 0.8532\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.1934 - accuracy: 0.9306\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.1339 - accuracy: 0.9542\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.1046 - accuracy: 0.9616\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_RNN_and_Attention_16.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "131uxv4-82L7LT1ENzRSjldbyEulIJjRm",
      "authorship_tag": "ABX9TyPogcxxOQj6nlYdS2EOeRKc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}