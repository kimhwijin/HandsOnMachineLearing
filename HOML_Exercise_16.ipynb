{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HOML_Exercise_16.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t--R2q2Skzwh",
        "akJLsnAjlwfR",
        "1z0AbCO79i0F",
        "rVtUzeUm9l6q",
        "O_tKelyG7zMc",
        "Yel4zMzBJjft",
        "atNGcKddjGCR"
      ],
      "mount_file_id": "1Fr4Ovc4aFGXpKVcgJ_lrNHnjtxWvrN3l",
      "authorship_tag": "ABX9TyOqnvGhFDPRckUMJBNSBHJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/HandsOnMachineLearing/blob/main/HOML_Exercise_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "wZFO65xLJ9j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "E8hNZpeOZvOV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "cOBH9WrNgMyU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8."
      ],
      "metadata": {
        "id": "-I_I4OsAaA8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grammer_StringGenerater"
      ],
      "metadata": {
        "id": "W3a5SCznh6zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#문법에 맞는 문자열을 반환하는 함수\n",
        "\n",
        "#문법, LR table 과 비슷함\n",
        "default_reber_grammar = [\n",
        "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
        "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
        "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
        "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
        "    [(\"X\", 3), (\"S\", 6)],\n",
        "    [(\"P\", 4), (\"V\", 6)],\n",
        "    #마지막\n",
        "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
        "\n",
        "embedded_reber_grammar = [\n",
        "    [(\"B\", 1)],\n",
        "    [(\"T\", 2), (\"P\", 3)],\n",
        "    [(default_reber_grammar, 4)],\n",
        "    [(default_reber_grammar, 5)],\n",
        "    [(\"T\", 6)],\n",
        "    [(\"P\", 6)],\n",
        "    #마지막\n",
        "    [(\"E\", None)]]\n",
        "\n",
        "def generate_string(grammar):\n",
        "    state = 0\n",
        "    output=[]\n",
        "    while state is not None:\n",
        "        index = np.random.randint(len(grammar[state]))\n",
        "        #다음 문자와 상태\n",
        "        production, state = grammar[state][index]\n",
        "        if isinstance(production, list):\n",
        "            production = generate_string(grammar=production)\n",
        "        output.append(production)\n",
        "    return \"\".join(output)"
      ],
      "metadata": {
        "id": "NTSOz5XNZ_Mh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(25):\n",
        "    #25개의 문자열 생성\n",
        "    print(generate_string(default_reber_grammar), end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeL37UA2a9ss",
        "outputId": "79a230c1-7091-4313-d9a4-adf67d560d6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wrong_StringGenrater"
      ],
      "metadata": {
        "id": "DNim0fuEiB2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모든 가능한 알파벳들\n",
        "POSSIBLE_CHARS = \"BEPSTVX\"\n",
        "\n",
        "#잘못된 문자열 만들기\n",
        "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
        "    #올바른 문자열에\n",
        "    good_string = generate_string(grammar)\n",
        "    index = np.random.randint(len(good_string))\n",
        "    good_char = good_string[index]\n",
        "    #있을수는 있지만 올바른 문자열엔 없는 문자열을\n",
        "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
        "    #중간에 하나 추가함\n",
        "    return good_string[:index] + bad_char + good_string[index + 1:]"
      ],
      "metadata": {
        "id": "zN88lc-qbMOH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#잘못된 문자열\n",
        "for _ in range(25):\n",
        "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QorG2sjibidd",
        "outputId": "b4374b9b-ab88-47b4-d855-1d636f83336a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTBPVVETV BTBTSSSPXXVVETE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE BPBPVVEPT BTBPTVEETE BTBTSSXXTTVXETE BTBTSXTTVVETE BPBPVVTPE BTBTSXTTVVETE EPBPVPXVVEPE BPTTXSEPE BPBTXXSPXTVVEPE BTBTXSPTE BPTTSXXTVPXVVEPE PPBPVPSEPE "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Char_To_Int"
      ],
      "metadata": {
        "id": "XzIk9HnZiIEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#문자열을 숫자로 바꾸기, 문자의 인덱스 리스트로 변경한다.\n",
        "def strings_to_ids(s, chars=POSSIBLE_CHARS):\n",
        "    return [chars.index(c) for c in s]"
      ],
      "metadata": {
        "id": "PrErPKNRgirg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataGenerater"
      ],
      "metadata": {
        "id": "ZDBhm6oJiN6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#올바른 문자열, 잘못된 문자열 1 : 1 인 샘플과 레이블 생성\n",
        "def generate_dataset(size):\n",
        "    good_strings = [strings_to_ids(generate_string(embedded_reber_grammar)) for _ in range(size // 2)]\n",
        "    bad_strings = [strings_to_ids(generate_corrupted_string(embedded_reber_grammar)) for _ in range(size // 2)]\n",
        "    all_strings = good_strings + bad_strings\n",
        "    #ragged 텐서 생성\n",
        "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
        "    y = np.array([[1.] for _ in range(len(good_strings))] + [[0.] for _ in range(len(bad_strings))])\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "wIQJCCtrb_6G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate_Data"
      ],
      "metadata": {
        "id": "HXsHV0dPiUAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = generate_dataset(10000)\n",
        "X_valid, y_valid = generate_dataset(2000)\n",
        "X_test, y_test = generate_dataset(2000)"
      ],
      "metadata": {
        "id": "MrWpQwZecAGU"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])\n",
        "s = ''\n",
        "for i in X_train[0]:\n",
        "    s += POSSIBLE_CHARS[i]\n",
        "print(s, y_train[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTjH-aNsgYpK",
        "outputId": "cb30c573-3b7a-4eb6-f2c5-ada50a9e837c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 4 0 2 5 5 1 4 1], shape=(9,), dtype=int32)\n",
            "BTBPVVETE 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary_Sequence_Classifier"
      ],
      "metadata": {
        "id": "UYff-SVGiYoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 5\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    #인풋데이터는 ragged tensor, int32 타입\n",
        "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
        "    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n",
        "    keras.layers.GRU(30),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "#네스테로프 가속 경사하강법 \n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.95, nesterov=True)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qfvhUibehqAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model_Train"
      ],
      "metadata": {
        "id": "XxCVuMK6kxut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-tLbiqajhY8",
        "outputId": "73324637-d686-4519-8c19-c12c4e20913a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 5), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 20s 55ms/step - loss: 0.6910 - accuracy: 0.5093 - val_loss: 0.6825 - val_accuracy: 0.5595\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.6678 - accuracy: 0.5720 - val_loss: 0.6706 - val_accuracy: 0.5975\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.6505 - accuracy: 0.5796 - val_loss: 0.6472 - val_accuracy: 0.6135\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 17s 56ms/step - loss: 0.6356 - accuracy: 0.5963 - val_loss: 0.6239 - val_accuracy: 0.6290\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.6059 - accuracy: 0.6342 - val_loss: 0.5778 - val_accuracy: 0.7010\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 17s 56ms/step - loss: 0.5474 - accuracy: 0.7015 - val_loss: 0.5608 - val_accuracy: 0.5500\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.3947 - accuracy: 0.8246 - val_loss: 0.2995 - val_accuracy: 0.8870\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.4298 - accuracy: 0.8124 - val_loss: 0.4701 - val_accuracy: 0.7770\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 0.3047 - accuracy: 0.8829 - val_loss: 0.2917 - val_accuracy: 0.8905\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.1288 - accuracy: 0.9661 - val_loss: 0.0894 - val_accuracy: 0.9785\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.0828 - accuracy: 0.9803 - val_loss: 0.0790 - val_accuracy: 0.9795\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.0720 - accuracy: 0.9825 - val_loss: 0.0728 - val_accuracy: 0.9815\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.0490 - accuracy: 0.9882 - val_loss: 0.0121 - val_accuracy: 0.9965\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.0085 - accuracy: 0.9987 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 8.7813e-04 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 8.8223e-04 - accuracy: 1.0000 - val_loss: 7.7920e-04 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 7.4183e-04 - accuracy: 1.0000 - val_loss: 6.4329e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model_Evaluate"
      ],
      "metadata": {
        "id": "t--R2q2Skzwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpS4OKE8kp_I",
        "outputId": "d4b793ef-72e8-4f76-bd54-ee467d52c2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 1s 9ms/step - loss: 6.7382e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0006738215452060103, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9."
      ],
      "metadata": {
        "id": "akJLsnAjlwfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9-1"
      ],
      "metadata": {
        "id": "1z0AbCO79i0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random_Date_Genrater"
      ],
      "metadata": {
        "id": "_6E6EwcLoalq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "\n",
        "#January 22, 2019 -> 2019-01-22 형식으로\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "\n",
        "    #숫자\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    #month, day, year 을 빼내기위해서\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "\n",
        "    #샘플\n",
        "    X = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
        "    #타깃 YYYY-mm-dd 형식으로 변환\n",
        "    y = [dt.isoformat() for dt in dates]\n",
        "    \n",
        "    return X, y"
      ],
      "metadata": {
        "id": "v27muj8wkCaA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_dates = 3\n",
        "x_example, y_example = random_dates(n_dates)\n",
        "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
        "print(\"-\" * 50)\n",
        "for idx in range(n_dates):\n",
        "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeR-1Hm5oFo_",
        "outputId": "a32817f0-dc89-4b9e-e92e-efc6e4785331"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input                    Target                   \n",
            "--------------------------------------------------\n",
            "July 22, 1344            1344-07-22               \n",
            "March 21, 7185           7185-03-21               \n",
            "January 02, 5192         5192-01-02               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INPUT_OUTPUT_CHARS"
      ],
      "metadata": {
        "id": "AjkPHuAct72Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
        "OUTPUT_CHARS = \"0123456789-\"\n",
        "INPUT_CHARS, OUTPUT_CHARS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP1YNS6XoZZw",
        "outputId": "cf2cfee9-c629-473f-c4aa-bf71f22de99c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' ,0123456789ADFJMNOSabceghilmnoprstuvy', '0123456789-')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input_Output_Tokenizer"
      ],
      "metadata": {
        "id": "UEhYuB4duC1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_chars = MONTHS + ['0123456789, ']\n",
        "output_chars = ['0123456789-']\n",
        "\n",
        "input_tokenizer = keras.preprocessing.text.Tokenizer(lower=False, char_level=True)\n",
        "input_tokenizer.fit_on_texts(sorted(set(input_chars)))\n",
        "input_vocab_size = len(input_tokenizer.word_index)\n",
        "\n",
        "output_tokenizer = keras.preprocessing.text.Tokenizer(lower=False, char_level=True)\n",
        "output_tokenizer.fit_on_texts(sorted(set(output_chars)))\n",
        "output_vocab_size = len(output_tokenizer.word_index)"
      ],
      "metadata": {
        "id": "OAsFfLXWo1Rp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_tokenizer.word_index)\n",
        "print(output_tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KizIjzmbqf0Q",
        "outputId": "b91605ee-8230-419a-c036-f974fd544070"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'e': 1, 'r': 2, 'u': 3, 'b': 4, 'a': 5, 'y': 6, 't': 7, 'c': 8, 'm': 9, 'J': 10, 'A': 11, 'p': 12, 'l': 13, 'n': 14, 'M': 15, 'o': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ',': 27, ' ': 28, 'i': 29, 'g': 30, 's': 31, 'D': 32, 'F': 33, 'h': 34, 'N': 35, 'v': 36, 'O': 37, 'S': 38}\n",
            "{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '-': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = random_dates(3)\n",
        "\n",
        "print('-'*25)\n",
        "print(X[0])\n",
        "X = input_tokenizer.texts_to_sequences(X)\n",
        "print(X[0])\n",
        "X = input_tokenizer.sequences_to_texts(X)\n",
        "print(X[0][::2])\n",
        "\n",
        "print('-'*25)\n",
        "\n",
        "print(y[0])\n",
        "y = output_tokenizer.texts_to_sequences(y)\n",
        "print(y[0])\n",
        "y = output_tokenizer.sequences_to_texts(y)\n",
        "print(y[0][::2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGWcYoYYtO6s",
        "outputId": "b2e98fa3-96c5-4c05-8bd9-25c9d20452b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "April 09, 3947\n",
            "[11, 12, 2, 29, 13, 28, 17, 26, 27, 28, 20, 26, 21, 24]\n",
            "April 09, 3947\n",
            "-------------------------\n",
            "3947-04-09\n",
            "[4, 10, 5, 8, 11, 1, 5, 11, 1, 10]\n",
            "3947-04-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset_Generater"
      ],
      "metadata": {
        "id": "7RPhEGkzuG--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_date_strs(date_strs, tokenizer):\n",
        "    ids = [dt for dt in tokenizer.texts_to_sequences(date_strs)]\n",
        "    X = tf.ragged.constant(ids, ragged_rank=1)\n",
        "    return X.to_tensor() #ragged 텐서를 기본 텐서로 부족한 부분은 0 패딩토큰으로 채운다.\n",
        "\n",
        "def create_dataset(n_dates):\n",
        "    X, Y = random_dates(n_dates)\n",
        "    return prepare_date_strs(X, input_tokenizer), prepare_date_strs(Y, output_tokenizer)"
      ],
      "metadata": {
        "id": "mZ2Zw8YKp2gs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = create_dataset(10000)\n",
        "X_valid, Y_valid = create_dataset(2000)\n",
        "X_test, Y_test = create_dataset(2000)"
      ],
      "metadata": {
        "id": "jPvrYD8DtDxW"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0], Y_train[0], sep='\\n')\n",
        "print(input_tokenizer.sequences_to_texts([X_train[0].numpy()])[0][::2])\n",
        "print(output_tokenizer.sequences_to_texts([Y_train[0].numpy()])[0][::2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGavgDWOtExG",
        "outputId": "f32d0d79-8a39-4b3e-ab7e-e3fadab3ee4e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([11 12  2 29 13 28 17 26 27 28 20 26 21 24  0  0  0  0], shape=(18,), dtype=int32)\n",
            "tf.Tensor([ 4 10  5  8 11  1  5 11  1 10], shape=(10,), dtype=int32)\n",
            "April 09, 3947\n",
            "3947-04-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Seq2Seq_Model"
      ],
      "metadata": {
        "id": "wdc1-KQzuLO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 32\n",
        "#출력 최대 길이 : 10 (yyyy-mm-dd)\n",
        "max_output_length = Y_train.shape[-1]\n",
        "\n",
        "#인코더 디코더 구조를 사용한다.\n",
        "\n",
        "#인코더, 시퀀스 투 벡터 구조\n",
        "encoder = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=input_vocab_size + 1, output_dim=embedding_size, input_shape=[None]),\n",
        "    keras.layers.LSTM(128)\n",
        "])\n",
        "\n",
        "decoder = keras.models.Sequential([\n",
        "    keras.layers.LSTM(128, return_sequences=True),\n",
        "    keras.layers.Dense(output_vocab_size + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    encoder,\n",
        "    #출력길이가 10이니깐 인코더 출력을 10번 반복해서, 디코더가 10개의 출력을 만들도록함\n",
        "    keras.layers.RepeatVector(max_output_length),\n",
        "    decoder\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u3NUSFrtE2-",
        "outputId": "e5ccf4bf-3506-4e06-9dea-99aabe817ce3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 128)               83680     \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 10, 128)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 10, 12)            133132    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 216,812\n",
            "Trainable params: 216,812\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yVNDBUquyezt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aic4qNg5ysUY",
        "outputId": "e3de6dd8-0739-40ae-b90e-25419e7840a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 10s 11ms/step - loss: 1.7915 - accuracy: 0.3509 - val_loss: 1.3838 - val_accuracy: 0.4813\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.6229 - accuracy: 0.4306 - val_loss: 1.2769 - val_accuracy: 0.5559\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.1316 - accuracy: 0.6030 - val_loss: 0.9796 - val_accuracy: 0.6527\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.9460 - accuracy: 0.6666 - val_loss: 0.7504 - val_accuracy: 0.7247\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.6254 - accuracy: 0.7651 - val_loss: 0.5397 - val_accuracy: 0.7903\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.7077 - accuracy: 0.7503 - val_loss: 0.4770 - val_accuracy: 0.8150\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.3763 - accuracy: 0.8550 - val_loss: 0.5772 - val_accuracy: 0.7980\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2678 - accuracy: 0.9050 - val_loss: 0.2131 - val_accuracy: 0.9329\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.1494 - accuracy: 0.9605 - val_loss: 0.1113 - val_accuracy: 0.9738\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0930 - accuracy: 0.9807 - val_loss: 0.0619 - val_accuracy: 0.9900\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0501 - accuracy: 0.9932 - val_loss: 0.0333 - val_accuracy: 0.9965\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0236 - accuracy: 0.9987 - val_loss: 0.0197 - val_accuracy: 0.9987\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0139 - accuracy: 0.9996 - val_loss: 0.0125 - val_accuracy: 0.9995\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0088 - accuracy: 0.9999 - val_loss: 0.0084 - val_accuracy: 0.9997\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 0.9999\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 0.9999\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9999\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 0.9999\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 0.9999\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vDkzHXF4UDN",
        "outputId": "31eab722-bf1e-42fc-fe35-73f57232e60f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0014783494407311082, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model_Return_To_Strings"
      ],
      "metadata": {
        "id": "U-kh3b2j4wwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ids_to_strings(ids, tokenizer):\n",
        "    return tokenizer.sequences_to_texts(ids)\n",
        "\n",
        "print('-'*25)\n",
        "#X_new = X_test[:3]\n",
        "#패딩이 없는 데이터셋이라 잘 예측을 못함\n",
        "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"], input_tokenizer)\n",
        "X_samples = ids_to_strings(X_new.numpy(), input_tokenizer)\n",
        "print(*[x_sample[::2] for x_sample in X_samples], sep='\\n')\n",
        "\n",
        "\n",
        "print('-'*25)\n",
        "Y_new = np.argmax(model.predict(X_new), axis=-1)\n",
        "Y_new = ids_to_strings(Y_new, output_tokenizer)\n",
        "print(*[y_sample[::2] for y_sample in Y_new], sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCAewYw44wTh",
        "outputId": "ddc5165d-df7f-4acc-aa96-2b23ecb667e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "May 02, 2020\n",
            "July 14, 1789\n",
            "-------------------------\n",
            "2020-01-02\n",
            "1789-01-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = X_train.shape[1]\n",
        "\n",
        "def prepare_date_strs_with_padded(date_strs, tokenizer):\n",
        "    X = prepare_date_strs(date_strs, tokenizer)\n",
        "    if X.shape[1] < max_input_length:\n",
        "        X = tf.pad(X, paddings=[[0,0], [0, max_input_length - X.shape[1]]])\n",
        "    return X\n",
        "\n",
        "\n",
        "print('-'*25)\n",
        "#패딩추가, 잘예측함\n",
        "X_new = prepare_date_strs_with_padded([\"May 02, 2020\", \"July 14, 1789\"], input_tokenizer)\n",
        "X_samples = ids_to_strings(X_new.numpy(), input_tokenizer)\n",
        "print(*[x_sample[::2] for x_sample in X_samples], sep='\\n')\n",
        "\n",
        "\n",
        "print('-'*25)\n",
        "Y_new = np.argmax(model.predict(X_new), axis=-1)\n",
        "Y_new = ids_to_strings(Y_new, output_tokenizer)\n",
        "print(*[y_sample[::2] for y_sample in Y_new], sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epYHWsGi79k0",
        "outputId": "24aac8b9-c48c-4572-e079-45400b80126f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "May 02, 2020\n",
            "July 14, 1789\n",
            "-------------------------\n",
            "2020-05-02\n",
            "1789-07-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9-2"
      ],
      "metadata": {
        "id": "rVtUzeUm9l6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder_Input_Target_Shift"
      ],
      "metadata": {
        "id": "codkA4LE-68c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sos_id = output_vocab_size + 1\n",
        "\n",
        "def shifted_output_sequences(Y):\n",
        "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
        "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
        "\n",
        "#타깃을 오른쪽으로 한번 shift 하고 맨앞에 sos 토큰을 추가한 디코더 입력\n",
        "#원래는 인코더벡터를 복사해서 디코더에 입력했지만, 쉬프트된 타깃을 주입해서 이전타깃이 무엇인지 확인할수 있게한다.\n",
        "X_train_decoder = shifted_output_sequences(Y_train)\n",
        "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
        "X_test_decoder = shifted_output_sequences(Y_test)"
      ],
      "metadata": {
        "id": "A_4wMFqR9nVj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_to_strings(X_train_decoder[:5].numpy(), output_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX978r__-kld",
        "outputId": "f0a601a4-1b60-4708-fcee-de44740838a6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3 9 4 7 - 0 4 - 0',\n",
              " '7 6 9 2 - 0 5 - 0',\n",
              " '5 0 4 3 - 0 2 - 2',\n",
              " '5 7 1 7 - 0 5 - 0',\n",
              " '3 7 6 2 - 0 6 - 1']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "0hp8YMJC_BC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "\n",
        "lstm_units = 128\n",
        "\n",
        "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "encoder_embedding = keras.layers.Embedding(input_dim=input_vocab_size+1, output_dim=encoder_embedding_size)(encoder_input)\n",
        "#인코더의 출력벡터는 무시한다!!\n",
        "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(lstm_units, return_state=True)(encoder_embedding)\n",
        "encoder_state = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "decoder_embedding = keras.layers.Embedding(output_vocab_size+1, decoder_embedding_size)(decoder_input)\n",
        "#대신 인코더의 LSTM의 상태와 디코더의 LSTM과 연결시킨다.\n",
        "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(decoder_embedding, initial_state=encoder_state)\n",
        "decoder_output = keras.layers.Dense(output_vocab_size+1, activation='softmax')(decoder_lstm_output)\n",
        "\n",
        "model = keras.models.Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7ZvSRmn-2XW",
        "outputId": "eab2c37f-e753-4cb0-9b02-4ebb2eb6ceef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 32)     1248        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 32)     384         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 128),        82432       ['embedding_2[0][0]']            \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  (None, None, 128)    82432       ['embedding_3[0][0]',            \n",
            "                                                                  'lstm_2[0][1]',                 \n",
            "                                                                  'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, None, 12)     1548        ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 168,044\n",
            "Trainable params: 168,044\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10, validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_HI_KKmAwLO",
        "outputId": "2d4ad6b4-20dc-4706-ddf2-d5197fdf3da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 7s 10ms/step - loss: 1.6702 - accuracy: 0.3712 - val_loss: 1.4184 - val_accuracy: 0.4505\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2351 - accuracy: 0.5254 - val_loss: 0.9711 - val_accuracy: 0.6467\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.6865 - accuracy: 0.7556 - val_loss: 0.3986 - val_accuracy: 0.8838\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2499 - accuracy: 0.9391 - val_loss: 0.1254 - val_accuracy: 0.9822\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0805 - accuracy: 0.9910 - val_loss: 0.0555 - val_accuracy: 0.9952\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0637 - accuracy: 0.9913 - val_loss: 0.0268 - val_accuracy: 0.9991\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0192 - accuracy: 0.9998 - val_loss: 0.0159 - val_accuracy: 0.9998\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ids_to_strings(np.argmax(model.predict([X_train[:5], X_train_decoder[:5]]), axis=-1), output_tokenizer))\n",
        "print(ids_to_strings(Y_train[:5].numpy(), output_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxR3yt-2HEWs",
        "outputId": "a354bb2e-cc68-47a9-8bbe-7dcdee23d30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['5 7 1 7 - 0 5 - 0 5', '3 7 6 2 - 0 6 - 1 3', '6 5 6 2 - 1 2 - 0 1', '3 9 8 7 - 0 3 - 0 4', '4 8 3 0 - 0 6 - 0 5']\n",
            "['5 7 1 7 - 0 5 - 0 5', '3 7 6 2 - 0 6 - 1 3', '6 5 6 2 - 1 2 - 0 1', '3 9 8 7 - 0 3 - 0 4', '4 8 3 0 - 0 6 - 0 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_new = output_tokenizer.texts_to_sequences([\"1789-07-14\", \"2020-05-01\"])\n",
        "\n",
        "def predict_date_strs(date_strs):\n",
        "    X = prepare_date_strs_with_padded(date_strs, input_tokenizer)\n",
        "    #X = date_strs\n",
        "    sos_id = output_vocab_size + 1\n",
        "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
        "\n",
        "    for index in range(max_output_length):\n",
        "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, max_output_length-index]])\n",
        "        Y_probas = tf.argmax(model.predict([X, X_decoder]), axis=-1, output_type=tf.int32)\n",
        "        Y_pred = tf.concat([Y_pred, Y_probas[:, index:index+1]], axis=1)\n",
        "    return ids_to_strings(Y_pred.numpy(), output_tokenizer)\n",
        "\n",
        "print(predict_date_strs([\"May 02, 2020\", \"July 14, 1789\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql1g0m9DHtk0",
        "outputId": "c0ed9c39-dc41-4058-aaf6-7c8937b998f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2 0 2 0 - 0 5 - 0 2', '1 7 8 9 - 0 7 - 1 4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9-3"
      ],
      "metadata": {
        "id": "O_tKelyG7zMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Addon_Sampler_Model"
      ],
      "metadata": {
        "id": "70us6ztfFHxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "\n",
        "units = 128\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "encoder_embeddings = keras.layers.Embedding(input_vocab_size + 1, encoder_embedding_size)(encoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(units, return_state=True)\n",
        "encoder_output, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "\n",
        "decoder_embedding_layer = keras.layers.Embedding(output_vocab_size + 2, decoder_embedding_size)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "#sampler\n",
        "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(units)\n",
        "output_layer = keras.layers.Dense(output_vocab_size+1)\n",
        "\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
        "\n",
        "final_output, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "Y_proba = keras.layers.Activation('softmax')(final_output.rnn_output)\n",
        "\n",
        "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Zb0E2Sdb70ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=9, validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LegU0AxjBJsv",
        "outputId": "539e61e3-ba4e-48cc-c5eb-1095c7555ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n",
            "313/313 [==============================] - 14s 33ms/step - loss: 1.6734 - accuracy: 0.3760 - val_loss: 1.4363 - val_accuracy: 0.4451\n",
            "Epoch 2/9\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 1.1483 - accuracy: 0.5837 - val_loss: 0.8829 - val_accuracy: 0.6852\n",
            "Epoch 3/9\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.5990 - accuracy: 0.7951 - val_loss: 0.3506 - val_accuracy: 0.8874\n",
            "Epoch 4/9\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.2181 - accuracy: 0.9460 - val_loss: 0.1073 - val_accuracy: 0.9862\n",
            "Epoch 5/9\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.0811 - accuracy: 0.9909 - val_loss: 0.0420 - val_accuracy: 0.9980\n",
            "Epoch 6/9\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0295 - accuracy: 0.9993 - val_loss: 0.0233 - val_accuracy: 0.9995\n",
            "Epoch 7/9\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0527 - accuracy: 0.9916 - val_loss: 0.0155 - val_accuracy: 0.9998\n",
            "Epoch 8/9\n",
            "313/313 [==============================] - 10s 30ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 0.9998\n",
            "Epoch 9/9\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_date_strs([\"May 02, 2020\", \"July 14, 1789\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDEEg6iVC1OJ",
        "outputId": "42f2c43b-c437-430b-b6e2-209a04ff0a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2 0 2 0 - 0 5 - 0 2', '1 7 8 9 - 0 7 - 1 4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GreedyEmbeddingSampler"
      ],
      "metadata": {
        "id": "XhyzO3P0FF7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(embedding_fn=decoder_embedding_layer)\n",
        "\n",
        "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, inference_sampler, output_layer=output_layer, maximum_iterations=max_output_length)\n",
        "batch_size = tf.shape(encoder_inputs)[:1]\n",
        "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
        "final_outputs, final_state, final_sequence_lengths = inference_decoder(start_tokens, initial_state=encoder_state, start_tokens=start_tokens, end_token=0)\n",
        "\n",
        "inference_model = keras.models.Model(inputs=[encoder_inputs], outputs=[final_outputs.sample_id])"
      ],
      "metadata": {
        "id": "q6pXkCJEHQRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_predict_date_strs(date_strs):\n",
        "    X = prepare_date_strs_with_padded(date_strs, input_tokenizer)\n",
        "    Y_pred = inference_model.predict(X)\n",
        "    return ids_to_strings(Y_pred, output_tokenizer)"
      ],
      "metadata": {
        "id": "sCRLYyTrHjei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "886WbTvKIGF1",
        "outputId": "b314efe0-1bd5-4ccc-e47a-b90af5c54f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 450 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS84z05UILLZ",
        "outputId": "f7955a0a-5218-4dff-dbbf-d90e21b84cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 13.47 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 41.8 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9-4"
      ],
      "metadata": {
        "id": "Yel4zMzBJjft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Schedule_Model"
      ],
      "metadata": {
        "id": "I8ONihUwLTiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "units = 128\n",
        "\n",
        "#encoder\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "encoder_embeddings = keras.layers.Embedding(input_vocab_size + 1, encoder_embedding_size)(encoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(units, return_state=True)\n",
        "encoder_output, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "#decoder\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\\\n",
        "\n",
        "decoder_embedding_layer = keras.layers.Embedding(output_vocab_size + 2, decoder_embedding_size)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "#sampler\n",
        "#훈련할때 점진적으로 타깃에서 예측한 값을 입력으로 보낸다.\n",
        "sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(sampling_probability=0., embedding_fn=decoder_embedding_layer)\n",
        "sampler.sampling_probability = tf.Variable(0.)\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(units)\n",
        "output_layer = keras.layers.Dense(output_vocab_size+1)\n",
        "\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
        "\n",
        "final_output, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "Y_proba = keras.layers.Activation('softmax')(final_output.rnn_output)\n",
        "\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "vOrjxr1jJlGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Schedule_Sampling_Probability_Callback"
      ],
      "metadata": {
        "id": "BP1TUkC4LObY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#매 에폭이 끝날때마다 예측한 값을 입력으로 보낼 확률을 올려줌\n",
        "def update_sampling_probability(epoch, logs):\n",
        "    proba = min(1.0, epoch / (n_epochs - 10))\n",
        "    sampler.sampling_probability.assign(proba)\n",
        "\n",
        "sampling_probability_cb = keras.callbacks.LambdaCallback(on_epoch_begin=update_sampling_probability)"
      ],
      "metadata": {
        "id": "CYzNqEi3KrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=n_epochs, \n",
        "                    validation_data=([X_valid, X_valid_decoder], Y_valid), \n",
        "                    callbacks=[sampling_probability_cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upq7F_zBK28w",
        "outputId": "02bec7e9-ed38-438b-b6b7-76c7dcc15c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 14s 33ms/step - loss: 0.0250 - accuracy: 0.9957 - val_loss: 0.0051 - val_accuracy: 0.9999\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 8.7203e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 6.5597e-04 - accuracy: 1.0000 - val_loss: 7.1708e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 5.0408e-04 - accuracy: 1.0000 - val_loss: 5.5726e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 10s 30ms/step - loss: 0.1314 - accuracy: 0.9699 - val_loss: 0.0056 - val_accuracy: 0.9998\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 0.9999\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 10s 30ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9999\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 10s 30ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9999\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 10s 30ms/step - loss: 8.5795e-04 - accuracy: 1.0000 - val_loss: 9.4585e-04 - val_accuracy: 0.9999\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 6.7363e-04 - accuracy: 1.0000 - val_loss: 7.5837e-04 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 5.4429e-04 - accuracy: 1.0000 - val_loss: 6.2861e-04 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 4.4745e-04 - accuracy: 1.0000 - val_loss: 5.1357e-04 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 3.7251e-04 - accuracy: 1.0000 - val_loss: 4.4432e-04 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 3.1222e-04 - accuracy: 1.0000 - val_loss: 3.6976e-04 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 2.6348e-04 - accuracy: 1.0000 - val_loss: 3.1881e-04 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 10s 30ms/step - loss: 2.2464e-04 - accuracy: 1.0000 - val_loss: 2.7817e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference_SampleEmbeddingSampler"
      ],
      "metadata": {
        "id": "a8mUMDlPL1TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_temperature = tf.Variable(1.)\n",
        "\n",
        "inference_sampler = tfa.seq2seq.sampler.SampleEmbeddingSampler(embedding_fn=decoder_embedding_layer, softmax_temperature=softmax_temperature)\n",
        "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, inference_sampler, output_layer=output_layer, maximum_iterations=max_output_length)\n",
        "\n",
        "batch_size = tf.shape(encoder_inputs)[:1]\n",
        "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
        "\n",
        "final_outputs, final_state, final_sequence_lengths = inference_decoder(start_tokens, initial_state=encoder_state, start_tokens=start_tokens, end_token=0)\n",
        "\n",
        "inference_model = keras.models.Model(inputs=[encoder_inputs], outputs=[final_outputs.sample_id])"
      ],
      "metadata": {
        "id": "hZ9l0idCL0rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creative_predict_date_strs(date_strs, temperature=1.0):\n",
        "    softmax_temperature.assign(temperature)\n",
        "    X = prepare_date_strs_with_padded(date_strs, input_tokenizer)\n",
        "    Y_pred = inference_model.predict(X)\n",
        "    return ids_to_strings(Y_pred, output_tokenizer)\n",
        "\n",
        "print(creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"]))\n",
        "print(creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"], 5.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLa2btjIMDtT",
        "outputId": "6e4c0982-fe09-462d-e092-86749881f280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0 4 1 7 3 4 8 3 5 8', '- 9 8 0 4 7 4 0 0']\n",
            "['9 1 9 2 1 4 2 6 0', '8 8 3 3 - 0 6 4 2 1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9-5"
      ],
      "metadata": {
        "id": "atNGcKddjGCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Subclassing_Model_Attention"
      ],
      "metadata": {
        "id": "6AdcCRAijJyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DateTranslate(keras.models.Model):\n",
        "    def __init__(self, units=128, encoder_embedding_size=32, decoder_embedding_size=32, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        #encoder\n",
        "        self.encoder_embedding = keras.layers.Embedding(input_dim=input_vocab_size+1, output_dim=encoder_embedding_size)\n",
        "        self.encoder = keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "        \n",
        "        #decoder\n",
        "        # - embedding\n",
        "        self.decoder_embedding = keras.layers.Embedding(input_dim=output_vocab_size+2, output_dim=decoder_embedding_size)\n",
        "        \n",
        "        # - attention\n",
        "        self.attention = tfa.seq2seq.LuongAttention(units)\n",
        "        decoder_inner_cell = keras.layers.LSTMCell(units)\n",
        "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(cell = decoder_inner_cell, attention_mechanism=self.attention)\n",
        "\n",
        "        # - basic decoder \n",
        "        output_layer = keras.layers.Dense(output_vocab_size+1)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(cell=self.decoder_cell, sampler=tfa.seq2seq.sampler.TrainingSampler(), output_layer=output_layer)\n",
        "        \n",
        "        # - inference decoder\n",
        "        self.inference_decoder = tfa.seq2seq.BasicDecoder(\n",
        "            cell=self.decoder_cell, \n",
        "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(embedding_fn=self.decoder_embedding),\n",
        "            output_layer=output_layer,\n",
        "            maximum_iterations=max_output_length\n",
        "            )\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "\n",
        "        encoder_input, decoder_input = inputs\n",
        "\n",
        "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
        "        encoder_outputs, state_h, state_c = self.encoder(encoder_embeddings, training=training)\n",
        "        encoder_state = [state_h, state_c]\n",
        "\n",
        "        self.attention(encoder_outputs, setup_memory=True)\n",
        "\n",
        "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
        "        decoder_initial_state = self.decoder_cell.get_initial_state(decoder_embeddings)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "        if training:\n",
        "            decoder_outputs, _, _ = self.decoder(decoder_embeddings, initial_state=decoder_initial_state, training=training)\n",
        "        else:\n",
        "            start_tokens = tf.zeros_like(encoder_input[:, 0]) + sos_id\n",
        "            decoder_outputs, _, _ = self.inference_decoder(decoder_embeddings, initial_state=decoder_initial_state, start_tokens=start_tokens, end_token=0)\n",
        "        \n",
        "        return tf.nn.softmax(decoder_outputs.rnn_output)"
      ],
      "metadata": {
        "id": "feGuNn1ujG6o"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DateTranslate()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DdC7tnssoVuv"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=25, validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy0x9y5HoeTt",
        "outputId": "42d4aa87-1358-4a22-d10f-80bd662b40dd"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "313/313 [==============================] - 19s 49ms/step - loss: 2.1546 - accuracy: 0.2259 - val_loss: 2.0625 - val_accuracy: 0.2520\n",
            "Epoch 2/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 1.8142 - accuracy: 0.3426 - val_loss: 1.3794 - val_accuracy: 0.4868\n",
            "Epoch 3/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 1.2884 - accuracy: 0.5156 - val_loss: 1.2056 - val_accuracy: 0.5426\n",
            "Epoch 4/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 1.4055 - accuracy: 0.4971 - val_loss: 1.3460 - val_accuracy: 0.5135\n",
            "Epoch 5/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 1.2818 - accuracy: 0.5337 - val_loss: 4.5334 - val_accuracy: 0.0550\n",
            "Epoch 6/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 1.1963 - accuracy: 0.5462 - val_loss: 1.1721 - val_accuracy: 0.5454\n",
            "Epoch 7/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 1.1563 - accuracy: 0.5514 - val_loss: 1.1474 - val_accuracy: 0.5561\n",
            "Epoch 8/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 1.1538 - accuracy: 0.5580 - val_loss: 1.1142 - val_accuracy: 0.5720\n",
            "Epoch 9/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 1.0435 - accuracy: 0.5927 - val_loss: 1.0687 - val_accuracy: 0.5989\n",
            "Epoch 10/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.8113 - accuracy: 0.6629 - val_loss: 0.9171 - val_accuracy: 0.6536\n",
            "Epoch 11/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.5813 - accuracy: 0.7441 - val_loss: 0.8241 - val_accuracy: 0.7135\n",
            "Epoch 12/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.3955 - accuracy: 0.8371 - val_loss: 0.4838 - val_accuracy: 0.8498\n",
            "Epoch 13/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 0.2807 - accuracy: 0.9080 - val_loss: 0.2931 - val_accuracy: 0.9288\n",
            "Epoch 14/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.1773 - accuracy: 0.9632 - val_loss: 0.1515 - val_accuracy: 0.9699\n",
            "Epoch 15/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.1176 - accuracy: 0.9801 - val_loss: 0.1672 - val_accuracy: 0.9651\n",
            "Epoch 16/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.0791 - accuracy: 0.9903 - val_loss: 0.0635 - val_accuracy: 0.9952\n",
            "Epoch 17/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.1038 - accuracy: 0.9814 - val_loss: 0.0636 - val_accuracy: 0.9913\n",
            "Epoch 18/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 0.0412 - accuracy: 0.9971 - val_loss: 0.0360 - val_accuracy: 0.9977\n",
            "Epoch 19/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 0.0298 - accuracy: 0.9978 - val_loss: 0.0244 - val_accuracy: 0.9997\n",
            "Epoch 20/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.0223 - accuracy: 0.9987 - val_loss: 0.0167 - val_accuracy: 0.9999\n",
            "Epoch 21/25\n",
            "313/313 [==============================] - 14s 46ms/step - loss: 0.0451 - accuracy: 0.9935 - val_loss: 0.0152 - val_accuracy: 0.9998\n",
            "Epoch 22/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.0120 - accuracy: 0.9999 - val_loss: 0.0117 - val_accuracy: 0.9999\n",
            "Epoch 23/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.0514 - accuracy: 0.9904 - val_loss: 0.0235 - val_accuracy: 0.9987\n",
            "Epoch 24/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.0115 - accuracy: 0.9999 - val_loss: 0.0099 - val_accuracy: 0.9999\n",
            "Epoch 25/25\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_predict_date_strs_v2(date_strs):\n",
        "    X = prepare_date_strs_with_padded(date_strs, input_tokenizer)\n",
        "    X_decoder = tf.zeros(shape=(len(X), max_output_length), dtype=tf.int32)\n",
        "    Y_probas = model.predict([X, X_decoder])\n",
        "    Y_pred = tf.argmax(Y_probas, axis=-1)\n",
        "    return ids_to_strings(Y_pred.numpy(), output_tokenizer)\n",
        "\n",
        "fast_predict_date_strs_v2([\"July 14, 1789\", \"May 01, 2020\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHSkFNWxVx_v",
        "outputId": "85add9e4-db09-4239-d1f9-089e1f79f4f5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1 7 8 9 - 0 7 - 1 4', '2 0 2 0 - 0 5 - 0 1']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10"
      ],
      "metadata": {
        "id": "tvTws_pmYFs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "metadata": {
        "id": "aEN1Ba5RYFH2"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip = keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIHzRUnlYL3S",
        "outputId": "48efdb4c-9222-4369-bec3-c537da5c8f4d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 유니코드 파일을 아스키 코드 파일로 변환합니다.\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # 단어와 단어 뒤에 오는 구두점(.)사이에 공백을 생성합니다.\n",
        "  # 예시: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # 참고:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")을 제외한 모든 것을 공백으로 대체합니다.\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해서\n",
        "  # 문장에 start와 end 토큰을 추가합니다.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "metadata": {
        "id": "dBBwufoBYawX"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path, num_examples):\n",
        "\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return zip(*word_pairs)"
      ],
      "metadata": {
        "id": "V4JONHG1a70f"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AttA7p9jcGvQ",
        "outputId": "1032c8f7-ec22-4180-e3a3-6e99bd04d42c"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    \n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "MwniMgEAcG19"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    target_lang, input_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, input_lang_tokenizer = tokenize(input_lang)\n",
        "    target_tensor, target_lang_tokenizer = tokenize(target_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer"
      ],
      "metadata": {
        "id": "SaGzE8vheg6q"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer = load_dataset(path_to_file, num_examples=num_examples)\n",
        "\n",
        "max_length_input, max_length_target = input_tensor.shape[-1], target_tensor.shape[-1]"
      ],
      "metadata": {
        "id": "LstNLY6ufKRD"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, Y_train, Y_valid = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "metadata": {
        "id": "cV_MRRcDfrl8"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tf.data.Dataset_Create"
      ],
      "metadata": {
        "id": "Th_9sCndgaDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_input_size = len(input_lang_tokenizer.word_index) + 1\n",
        "vocab_target_size = len(target_lang_tokenizer.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(1)"
      ],
      "metadata": {
        "id": "j8M5c3JpgNjE"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_input_size, vocab_target_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcSkdSnEhJvS",
        "outputId": "5ef57876-3dab-4a03-9ed4-f4eb0d6ca72b"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9414, 4935)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(keras.models.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder_units = encoder_units\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(self.encoder_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, input, hidden_state):\n",
        "        x = self.embedding(input)\n",
        "        output, state = self.gru(x, initial_state=hidden_state)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.encoder_units))"
      ],
      "metadata": {
        "id": "mMJsPKl8i6Zw"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahadanauAttention(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        #score : FC(tanh(FC(EO) + FC(H)))\n",
        "\n",
        "        #query hidden state : (batch_size, hidden_size)\n",
        "        #query_with_time_axis : (batch_size, 1, hidden_size)\n",
        "        #values : (batch_size, max_len, hidden_size)\n",
        "        query_with_time_axis = tf.expand_dims(query, axis=1)\n",
        "\n",
        "        #score : (batch_size, max_len, 1)\n",
        "        score = self.V(\n",
        "            tf.nn.tanh(\n",
        "                #(batch_size, max_len, units)\n",
        "                self.W1(query_with_time_axis) + self.W2(values)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        #(batch_size, max_len, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        #가중치 합\n",
        "        #context_vector : (batch_size, hidden_size)\n",
        "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "ZeSpgIeQkFyE"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(keras.models.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(self.decoder_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "        self.fc = keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahadanauAttention(self.decoder_units)\n",
        "\n",
        "    def call(self, x, hidden_state, encoder_output):\n",
        "        #x : (batch_size, 1)\n",
        "        #hidden_state : (batch_size, 1, hidden_size)\n",
        "        #encoder_output : (batch_size, max_len, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden_state, encoder_output)\n",
        "\n",
        "        #embedded x : (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        #concated x : (batch_size, 1, hidden_size + embedding_dim)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis=-1)\n",
        "\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "metadata": {
        "id": "6E-xv0y2qaX3"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_target_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_input, sample_target = next(iter(dataset.take(1)))\n",
        "print(\"input, target shape : (batch_size, max_input_len), (batch_size, max_target_len)\",sample_input.shape, sample_target.shape)\n",
        "encoder_output, hidden_state = encoder(sample_input, encoder.initialize_hidden_state())\n",
        "print(\"encoder output : (batch_size, max_input_len, units)\", encoder_output.shape)\n",
        "print(\"hidden state : (batch_size, units)\", hidden_state.shape)\n",
        "decoder_output, hidden_state, attention_weights = decoder(tf.random.uniform((BATCH_SIZE, 1)), hidden_state, encoder_output)\n",
        "print(\"decoder output : (batch_size, vocab_output_size)\", decoder_output.shape)\n",
        "print(\"decoder state : (batch_size, decoder_units)\", hidden_state.shape)\n",
        "print(\"attention weights : (batch_size, max_input_len, 1)\", attention_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDd4WButu_wT",
        "outputId": "afd763b4-c6b3-481c-9ca3-889013596f36"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input, target shape : (batch_size, max_input_len), (batch_size, max_target_len) (64, 16) (64, 11)\n",
            "TensorShape([64, 16, 256])\n",
            "encoder output : (batch_size, max_input_len, units) (64, 16, 1024)\n",
            "hidden state : (batch_size, units) (64, 1024)\n",
            "decoder output : (batch_size, vocab_output_size) (64, 4935)\n",
            "decoder state : (batch_size, decoder_units) (64, 1024)\n",
            "attention weights : (batch_size, max_input_len, 1) (64, 16, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_fn(y_real, y_pred):\n",
        "    #타깃이 패딩 토큰이면 무시하는 마스킹 생성\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n",
        "    loss_ = loss_object(y_real, y_pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "msVUqcAbtkQ6"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_model_path = 'drive/MyDrive/Model/spa2eng'\n",
        "checkpoint_dir = drive_model_path + '/training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "fPoUMhtWugWp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}